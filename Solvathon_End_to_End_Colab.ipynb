{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Solvathon Layer-1 End-to-End Colab Notebook\n\nThis notebook builds and demonstrates the complete voice-agent flow in one place:\n\n1. Install dependencies.\n2. Build each component (LID, STT, Agent Router, TTS).\n3. Run component test cases inside the notebook.\n4. Run integrated single-turn pipeline tests.\n5. Launch a press-and-hold website.\n6. Expose it publicly with ngrok for two-way voice interaction.\n\nTarget behavior implemented:\n- Press and hold record button -> recording starts.\n- Release button -> recorded chunk uploads to server.\n- Server returns synthesized voice reply -> browser auto-plays.\n- Purpose selector: `Hospital Kiosk`, `College Admission`, `Laptop Customer Support`.\n- Language selector before speaking.\n- TTS policy: Piper for `en/hi/te/ta` (Tamil with your trained IITM model), Edge TTS for `kn`.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Install Dependencies"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "%%capture\n!apt-get -qq update\n!apt-get -qq install -y ffmpeg\n!pip -q install flask flask-cors pyngrok numpy librosa soundfile scipy requests\n!pip -q install torch transformers sentencepiece accelerate\n!pip -q install piper-tts edge-tts av nest_asyncio"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import torch\nprint('Torch version:', torch.__version__)\nprint('CUDA available:', torch.cuda.is_available())\nif torch.cuda.is_available():\n    print('GPU:', torch.cuda.get_device_name(0))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Global Setup and Config"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os\nimport io\nimport av\nimport json\nimport time\nimport base64\nimport shutil\nimport asyncio\nimport random\nimport requests\nimport subprocess\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom pathlib import Path\nfrom collections import defaultdict\n\nimport nest_asyncio\nnest_asyncio.apply()\n\nfrom IPython.display import Audio, display, HTML\nfrom transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\n\nROOT = Path('/content/solvathon_colab')\nMODELS_DIR = ROOT / 'piper_models'\nTEST_AUDIO_DIR = ROOT / 'test_audio'\nOUTPUT_DIR = ROOT / 'outputs'\nfor p in [ROOT, MODELS_DIR, TEST_AUDIO_DIR, OUTPUT_DIR]:\n    p.mkdir(parents=True, exist_ok=True)\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Working directory:', ROOT)\nprint('Device:', DEVICE)\n\nLANGUAGE_CONFIG = {\n    'en': {\n        'name': 'English',\n        'edge_voice': 'en-US-AriaNeural',\n        'test_text': 'Hello, I need quick help from the voice assistant.'\n    },\n    'hi': {\n        'name': 'Hindi',\n        'edge_voice': 'hi-IN-SwaraNeural',\n        'test_text': 'नमस्ते, मुझे वॉयस असिस्टेंट से मदद चाहिए।'\n    },\n    'ta': {\n        'name': 'Tamil',\n        'edge_voice': 'ta-IN-PallaviNeural',\n        'test_text': 'வணக்கம், எனக்கு குரல் உதவி வேண்டும்.'\n    },\n    'te': {\n        'name': 'Telugu',\n        'edge_voice': 'te-IN-MohanNeural',\n        'test_text': 'హలో, నాకు వాయిస్ సహాయం కావాలి.'\n    },\n    'kn': {\n        'name': 'Kannada',\n        'edge_voice': 'kn-IN-SapnaNeural',\n        'test_text': 'ನಮಸ್ಕಾರ, ನನಗೆ ಧ್ವನಿ ಸಹಾಯ ಬೇಕು.'\n    },\n}\n\nAGENT_TYPES = {\n    'hospital_kiosk': {\n        'label': 'Hospital Kiosk',\n        'assumption': 'You are speaking to a hospital front-desk voice kiosk.'\n    },\n    'college_admission': {\n        'label': 'College Admission',\n        'assumption': 'You are speaking to a college admission help desk voice agent.'\n    },\n    'laptop_support': {\n        'label': 'Laptop Customer Support',\n        'assumption': 'You are speaking to a laptop technical support voice agent.'\n    },\n}\n\nSESSION_MEMORY = defaultdict(list)\nprint('Configured languages:', ', '.join(LANGUAGE_CONFIG.keys()))\nprint('Configured agent types:', ', '.join(AGENT_TYPES.keys()))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Download/Prepare Piper Models (en/hi/te required, ta custom)\n\nTamil uses your trained model: `ta_IN-iitm-female-s1-medium.onnx`.\nIf this file is not already in Colab, upload it (and json) first.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "PIPER_BASE = 'https://huggingface.co/rhasspy/piper-voices/resolve/main'\n\n\ndef download_file(url: str, target: Path, required: bool = True) -> bool:\n    target.parent.mkdir(parents=True, exist_ok=True)\n    r = requests.get(url, timeout=120)\n    if r.status_code == 200:\n        target.write_bytes(r.content)\n        return True\n    if required:\n        raise RuntimeError(f'Failed to download required file: {url} (status={r.status_code})')\n    return False\n\n\ndef download_piper_voice(lang: str, region: str, name: str, quality: str = 'medium', required: bool = True) -> bool:\n    stem = f'{lang}_{region}-{name}-{quality}'\n    onnx = f'{stem}.onnx'\n    meta = f'{stem}.onnx.json'\n    voice_path = f'{lang}/{lang}_{region}/{name}/{quality}'\n    ok_onnx = download_file(f'{PIPER_BASE}/{voice_path}/{onnx}', MODELS_DIR / onnx, required=required)\n    if ok_onnx:\n        download_file(f'{PIPER_BASE}/{voice_path}/{meta}', MODELS_DIR / meta, required=False)\n        print('Downloaded:', onnx)\n        return True\n    return False\n\n# Required voices\n_ = download_piper_voice('en', 'US', 'lessac', 'medium', required=True)\n_ = download_piper_voice('hi', 'IN', 'rohan', 'medium', required=True)\n_ = download_piper_voice('te', 'IN', 'maya', 'medium', required=True)\n\n# Tamil custom model: preferred\ncustom_ta = Path('/content/ta_IN-iitm-female-s1-medium.onnx')\ncustom_ta_json = Path('/content/ta_IN-iitm-female-s1-medium.onnx.json')\nif custom_ta.exists():\n    shutil.copy2(custom_ta, MODELS_DIR / custom_ta.name)\n    if custom_ta_json.exists():\n        shutil.copy2(custom_ta_json, MODELS_DIR / custom_ta_json.name)\n    print('Copied custom Tamil model into', MODELS_DIR)\nelse:\n    print('Custom Tamil model not found in /content. Upload ta_IN-iitm-female-s1-medium.onnx for Piper Tamil.')\n\nprint('\nAvailable Piper models:')\nfor f in sorted(MODELS_DIR.glob('*.onnx')):\n    print('-', f.name)\n\nprint('Piper binary:', shutil.which('piper'))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Build Component: Language Identification (MMS-LID)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "LID_MODEL_NAME = 'facebook/mms-lid-256'\n\nlid_processor = AutoFeatureExtractor.from_pretrained(LID_MODEL_NAME)\nlid_model = Wav2Vec2ForSequenceClassification.from_pretrained(LID_MODEL_NAME).to(DEVICE)\nlid_model.eval()\nlid_id2label = lid_model.config.id2label\n\nLANG_MAP = {\n    'eng': 'en',\n    'hin': 'hi',\n    'tam': 'ta',\n    'tel': 'te',\n    'kan': 'kn',\n}\nTARGET_CODES = set(LANG_MAP.keys())\n\n\ndef detect_language_from_array(audio: np.ndarray, sr: int = 16000):\n    if sr != 16000:\n        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n    audio = audio.astype(np.float32)\n\n    inputs = lid_processor(audio, sampling_rate=16000, return_tensors='pt')\n    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        logits = lid_model(**inputs).logits\n        probs = torch.softmax(logits, dim=-1)[0].detach().cpu().numpy()\n\n    ranked = sorted(\n        [(lid_id2label[i], float(probs[i])) for i in range(len(probs))],\n        key=lambda x: x[1],\n        reverse=True,\n    )\n\n    filtered = [(code, p) for code, p in ranked if code in TARGET_CODES]\n    if not filtered:\n        return 'en', 0.0, ranked[:5]\n\n    best_code, best_prob = filtered[0]\n    return LANG_MAP.get(best_code, 'en'), best_prob, filtered[:5]\n\n\ndef detect_language_from_file(path: Path):\n    wav, sr = librosa.load(str(path), sr=16000, mono=True)\n    return detect_language_from_array(wav, sr=16000)\n\nprint('LID loaded on', DEVICE)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### LID Test Cases (generated inside notebook)\n\nThis generates one short sample per language (using Edge voices), then runs MMS-LID.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import edge_tts\n\nasync def edge_save_wav(text: str, voice: str, out_wav: Path):\n    out_wav.parent.mkdir(parents=True, exist_ok=True)\n    tmp_mp3 = out_wav.with_suffix('.mp3')\n    comm = edge_tts.Communicate(text=text, voice=voice)\n    await comm.save(str(tmp_mp3))\n    subprocess.run([\n        'ffmpeg', '-y', '-loglevel', 'error', '-i', str(tmp_mp3), '-ac', '1', '-ar', '16000', str(out_wav)\n    ], check=True)\n    tmp_mp3.unlink(missing_ok=True)\n\nfor code, cfg in LANGUAGE_CONFIG.items():\n    out = TEST_AUDIO_DIR / f'lid_{code}.wav'\n    try:\n        asyncio.get_event_loop().run_until_complete(edge_save_wav(cfg['test_text'], cfg['edge_voice'], out))\n        print('Generated:', out.name)\n    except Exception as e:\n        print('Generation failed for', code, '-', e)\n\nprint('\nRunning LID predictions:')\nfor wav_path in sorted(TEST_AUDIO_DIR.glob('lid_*.wav')):\n    pred, conf, top5 = detect_language_from_file(wav_path)\n    print(f'{wav_path.name:16s} -> pred={pred}, conf={conf:.3f}, top={top5[:3]}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Build Component: STT (Whisper)\n\nUsing `openai/whisper-small` to transcribe uploaded user audio.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "WHISPER_MODEL_NAME = 'openai/whisper-small'\n\nwhisper_processor = WhisperProcessor.from_pretrained(WHISPER_MODEL_NAME)\nwhisper_model = WhisperForConditionalGeneration.from_pretrained(WHISPER_MODEL_NAME).to(DEVICE)\nwhisper_model.eval()\n\n\ndef transcribe_audio_array(audio: np.ndarray, sr: int = 16000, language: str = 'en') -> str:\n    if sr != 16000:\n        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n    audio = np.clip(audio.astype(np.float32), -1.0, 1.0)\n\n    inputs = whisper_processor(audio, sampling_rate=16000, return_tensors='pt')\n    forced_ids = None\n    try:\n        forced_ids = whisper_processor.get_decoder_prompt_ids(language=language, task='transcribe')\n    except Exception:\n        forced_ids = None\n\n    kwargs = {'forced_decoder_ids': forced_ids} if forced_ids else {}\n    with torch.no_grad():\n        generated = whisper_model.generate(inputs.input_features.to(DEVICE), **kwargs)\n    text = whisper_processor.batch_decode(generated, skip_special_tokens=True)[0].strip()\n    return text\n\n\ndef transcribe_audio_file(path: Path, language: str = 'en') -> str:\n    wav, sr = librosa.load(str(path), sr=16000, mono=True)\n    return transcribe_audio_array(wav, sr=16000, language=language)\n\nprint('Whisper loaded on', DEVICE)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "print('STT test outputs:')\nfor wav_path in sorted(TEST_AUDIO_DIR.glob('lid_*.wav')):\n    lang_code = wav_path.stem.split('_')[-1]\n    try:\n        text = transcribe_audio_file(wav_path, language=lang_code if lang_code != 'auto' else 'en')\n        print(f'{wav_path.name:16s} -> {text}')\n    except Exception as e:\n        print(f'{wav_path.name:16s} -> ERROR: {e}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Build Component: Purpose-Aware Voice Agent Logic\n\nAssumption-based response generation aligned to selected agent purpose.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "REPLY_LIBRARY = {\n    'hospital_kiosk': {\n        'en': 'I am the hospital kiosk assistant. Please share age, key symptom, and how long it has been present. If breathing pain or heavy bleeding is present, go to emergency desk now.',\n        'hi': 'मैं अस्पताल कियोस्क सहायक हूँ। कृपया उम्र, मुख्य लक्षण और कितने समय से समस्या है बताइए। सांस की तकलीफ या ज्यादा bleeding हो तो तुरंत emergency desk पर जाएँ।',\n        'ta': 'நான் மருத்துவமனை கியோஸ்க் உதவியாளர். வயது, முக்கிய அறிகுறி, எத்தனை நாளாக உள்ளது என்பதை சொல்லுங்கள். மூச்சுத்திணறல் அல்லது கடும் ரத்தப்போக்கு இருந்தால் உடனே அவசர பிரிவுக்கு செல்லுங்கள்.',\n        'te': 'నేను హాస్పిటల్ కియోస్క్ అసిస్టెంట్‌ను. వయసు, ప్రధాన లక్షణం, ఎంతకాలంగా ఉందో చెప్పండి. శ్వాస ఇబ్బంది లేదా ఎక్కువ రక్తస్రావం ఉంటే వెంటనే ఎమర్జెన్సీ డెస్క్‌కు వెళ్లండి.',\n        'kn': 'ನಾನು ಆಸ್ಪತ್ರೆ ಕಿಯಾಸ್ಕ್ ಸಹಾಯಕ. ವಯಸ್ಸು, ಮುಖ್ಯ ಲಕ್ಷಣ, ಎಷ್ಟು ದಿನದಿಂದ ಇದೆ ಎಂದು ಹೇಳಿ. ಉಸಿರಾಟ ಕಷ್ಟ ಅಥವಾ ಹೆಚ್ಚು ರಕ್ತಸ್ರಾವ ಇದ್ದರೆ ತಕ್ಷಣ ತುರ್ತು ವಿಭಾಗಕ್ಕೆ ಹೋಗಿ.'\n    },\n    'college_admission': {\n        'en': 'I am the college admission assistant. I can guide eligibility, documents, fee estimate, and timeline. Please share your course and board marks.',\n        'hi': 'मैं कॉलेज एडमिशन सहायक हूँ। मैं eligibility, documents, fee estimate और timeline बताऊँगा। कृपया course और marks बताइए।',\n        'ta': 'நான் கல்லூரி சேர்க்கை உதவியாளர். தகுதி, ஆவணங்கள், கட்டண மதிப்பீடு மற்றும் கால அட்டவணையில் வழிகாட்டுவேன். உங்கள் பாடநெறி மற்றும் மதிப்பெண்களை சொல்லுங்கள்.',\n        'te': 'నేను కాలేజ్ అడ్మిషన్ అసిస్టెంట్‌ను. అర్హత, డాక్యుమెంట్లు, ఫీజు అంచనా, టైమ్‌లైన్ గురించి సహాయం చేస్తాను. మీ కోర్సు మరియు మార్కులు చెప్పండి.',\n        'kn': 'ನಾನು ಕಾಲೇಜು ಪ್ರವೇಶ ಸಹಾಯಕ. ಅರ್ಹತೆ, ದಾಖಲೆಗಳು, ಶುಲ್ಕ ಅಂದಾಜು ಮತ್ತು ಸಮಯರೇಖೆಯಲ್ಲಿ ಸಹಾಯ ಮಾಡುತ್ತೇನೆ. ನಿಮ್ಮ ಕೋರ್ಸ್ ಮತ್ತು ಅಂಕಗಳನ್ನು ತಿಳಿಸಿ.'\n    },\n    'laptop_support': {\n        'en': 'I am laptop support assistant. Please share laptop model, operating system, and exact issue. I will give safe step-by-step troubleshooting.',\n        'hi': 'मैं लैपटॉप सपोर्ट सहायक हूँ। कृपया laptop model, operating system और exact issue बताइए। मैं safe step-by-step troubleshooting दूँगा।',\n        'ta': 'நான் லாப்டாப் ஆதரவு உதவியாளர். லாப்டாப் மாடல், operating system, மற்றும் சரியான பிரச்சினையை சொல்லுங்கள். பாதுகாப்பான படிப்படியான troubleshooting கொடுக்கிறேன்.',\n        'te': 'నేను ల్యాప్‌టాప్ సపోర్ట్ అసిస్టెంట్‌ను. ల్యాప్‌టాప్ మోడల్, operating system, ఖచ్చితమైన సమస్య చెప్పండి. సురక్షితంగా దశలవారీ troubleshooting ఇస్తాను.',\n        'kn': 'ನಾನು ಲ್ಯಾಪ್‌ಟಾಪ್ ಸಹಾಯ ಸಹಾಯಕ. ಲ್ಯಾಪ್‌ಟಾಪ್ ಮಾದರಿ, operating system, ನಿಖರ ಸಮಸ್ಯೆ ಹೇಳಿ. ಸುರಕ್ಷಿತ ಹಂತ-ಹಂತದ troubleshooting ನೀಡುತ್ತೇನೆ.'\n    }\n}\n\n\ndef generate_agent_response(user_text: str, agent_type: str, language: str, session_id: str = 'demo') -> str:\n    agent_type = agent_type if agent_type in AGENT_TYPES else 'hospital_kiosk'\n    language = language if language in LANGUAGE_CONFIG else 'en'\n\n    base = REPLY_LIBRARY[agent_type].get(language, REPLY_LIBRARY[agent_type]['en'])\n\n    history = SESSION_MEMORY[session_id]\n    history.append({'user': user_text, 'agent_type': agent_type, 'lang': language})\n    if len(history) > 4:\n        SESSION_MEMORY[session_id] = history[-4:]\n\n    if len(history) > 1:\n        if language == 'en':\n            base += ' I am continuing from your previous question in this same session.'\n        elif language == 'hi':\n            base += ' मैं इसी session में आपके पिछले सवाल को ध्यान में रखकर जवाब दे रहा हूँ।'\n        elif language == 'ta':\n            base += ' இதே session-இல் உங்கள் முந்தைய கேள்வியை வைத்து தொடர்கிறேன்.'\n        elif language == 'te':\n            base += ' ఇదే session‌లో మీ ముందటి ప్రశ్నను తీసుకుని కొనసాగిస్తున్నాను.'\n        elif language == 'kn':\n            base += ' ಇದೇ session‌ನಲ್ಲಿ ನಿಮ್ಮ ಹಿಂದಿನ ಪ್ರಶ್ನೆಯನ್ನು ಗಮನಿಸಿ ಮುಂದುವರಿಸುತ್ತಿದ್ದೇನೆ.'\n\n    return base\n\nagent_tests = [\n    ('hospital_kiosk', 'en', 'My father has chest pain for 20 minutes.'),\n    ('college_admission', 'hi', 'BTech computer science ke liye kya documents chahiye?'),\n    ('laptop_support', 'ta', 'என் laptop அடிக்கடி restart ஆகிறது.'),\n]\nfor a, l, q in agent_tests:\n    print(f'[{a} | {l}] Q: {q}')\n    print('A:', generate_agent_response(q, a, l, session_id='agent_test'))\n    print('-' * 100)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Build Component: TTS (Piper + Edge Policy)\n\nPolicy:\n- `kn` -> Edge TTS\n- `en/hi/te/ta` -> Piper if model exists\n- fallback to Edge TTS\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import edge_tts\n\n\ndef piper_model_candidates(lang: str):\n    table = {\n        'en': ['en_US-lessac-medium.onnx'],\n        'hi': ['hi_IN-rohan-medium.onnx'],\n        'te': ['te_IN-maya-medium.onnx'],\n        'ta': ['ta_IN-iitm-female-s1-medium.onnx', 'ta_IN-kani-medium.onnx', 'ta_IN-ponni-medium.onnx', 'ta_IN-tamil-medium.onnx'],\n    }\n    return table.get(lang, [])\n\n\ndef pick_piper_model(lang: str):\n    for name in piper_model_candidates(lang):\n        path = MODELS_DIR / name\n        if path.exists():\n            return path\n    return None\n\n\nasync def edge_tts_to_wav(text: str, lang: str, out_wav: Path):\n    voice = LANGUAGE_CONFIG.get(lang, LANGUAGE_CONFIG['en'])['edge_voice']\n    tmp_mp3 = out_wav.with_suffix('.mp3')\n    comm = edge_tts.Communicate(text=text, voice=voice)\n    await comm.save(str(tmp_mp3))\n    subprocess.run(['ffmpeg', '-y', '-loglevel', 'error', '-i', str(tmp_mp3), '-ac', '1', '-ar', '48000', str(out_wav)], check=True)\n    tmp_mp3.unlink(missing_ok=True)\n\n\ndef piper_tts_to_wav(text: str, model_path: Path, out_wav: Path):\n    piper_bin = shutil.which('piper')\n    if not piper_bin:\n        raise RuntimeError('piper binary not found')\n    proc = subprocess.run(\n        [piper_bin, '--model', str(model_path), '--output_file', str(out_wav)],\n        input=text.encode('utf-8'),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        check=False,\n    )\n    if proc.returncode != 0:\n        raise RuntimeError(proc.stderr.decode('utf-8', errors='ignore'))\n\n\ndef synthesize_tts(text: str, lang: str, tag: str = 'reply'):\n    out_wav = OUTPUT_DIR / f'{tag}_{int(time.time() * 1000)}_{random.randint(1000,9999)}.wav'\n\n    engine = 'edge'\n    if lang == 'kn':\n        engine = 'edge'\n    elif lang in {'en', 'hi', 'te', 'ta'} and pick_piper_model(lang) is not None:\n        engine = 'piper'\n    elif lang in {'en', 'hi', 'te', 'ta'}:\n        engine = 'edge'\n\n    if engine == 'piper':\n        model = pick_piper_model(lang)\n        piper_tts_to_wav(text, model, out_wav)\n    else:\n        asyncio.get_event_loop().run_until_complete(edge_tts_to_wav(text, lang, out_wav))\n\n    return out_wav, engine\n\nfor lang in ['en', 'hi', 'ta', 'te', 'kn']:\n    text = LANGUAGE_CONFIG[lang]['test_text']\n    wav_path, engine = synthesize_tts(text, lang, tag=f'tts_{lang}')\n    print(f'{lang} -> engine={engine}, file={wav_path.name}')\n    display(Audio(filename=str(wav_path), autoplay=False))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Integrate End-to-End Single-Turn Pipeline"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def decode_audio_bytes_to_float32(audio_bytes: bytes, target_sr: int = 16000):\n    try:\n        audio, sr = sf.read(io.BytesIO(audio_bytes), dtype='float32')\n        if audio.ndim > 1:\n            audio = np.mean(audio, axis=1)\n        if sr != target_sr:\n            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n        return audio.astype(np.float32), target_sr\n    except Exception:\n        container = av.open(io.BytesIO(audio_bytes))\n        resampler = av.AudioResampler(format='s16', layout='mono', rate=target_sr)\n        pcm = bytearray()\n        for frame in container.decode(audio=0):\n            for resampled in resampler.resample(frame):\n                pcm += bytes(resampled.planes[0])\n        if not pcm:\n            raise RuntimeError('Could not decode uploaded audio bytes')\n        audio = np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0\n        return audio, target_sr\n\n\ndef run_single_turn(audio_bytes: bytes, language: str = 'auto', agent_type: str = 'hospital_kiosk', session_id: str = 'demo_session'):\n    audio_16k, sr = decode_audio_bytes_to_float32(audio_bytes, target_sr=16000)\n\n    if language == 'auto':\n        detected, conf, _ = detect_language_from_array(audio_16k, sr=16000)\n        lang = detected if conf >= 0.35 else 'en'\n    else:\n        lang = language if language in LANGUAGE_CONFIG else 'en'\n\n    transcript = transcribe_audio_array(audio_16k, sr=16000, language=lang)\n    response = generate_agent_response(transcript or 'No speech captured.', agent_type, lang, session_id=session_id)\n    reply_wav, tts_engine = synthesize_tts(response, lang, tag='turn')\n\n    reply_bytes = reply_wav.read_bytes()\n\n    return {\n        'language': lang,\n        'transcript': transcript,\n        'response': response,\n        'tts_engine': tts_engine,\n        'reply_wav_path': str(reply_wav),\n        'reply_audio_b64': base64.b64encode(reply_bytes).decode('ascii')\n    }\n\n# End-to-end test using generated sample input (English)\ntest_in = TEST_AUDIO_DIR / 'lid_en.wav'\nif test_in.exists():\n    result = run_single_turn(test_in.read_bytes(), language='auto', agent_type='hospital_kiosk', session_id='pipeline_demo')\n    print(json.dumps({k: v for k, v in result.items() if k != 'reply_audio_b64'}, indent=2, ensure_ascii=False))\n    display(Audio(filename=result['reply_wav_path'], autoplay=False))\nelse:\n    print('No test input found at', test_in)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) Build Website + API Server (Press and Hold)\n\nThis app exposes:\n- `GET /` -> press-and-hold web page\n- `GET /healthz`\n- `GET /agent-types`\n- `POST /voice-turn`\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from flask import Flask, request, jsonify\nfrom flask_cors import CORS\nimport threading\n\nINDEX_HTML = r'''\n<!doctype html>\n<html>\n<head>\n  <meta charset=\"utf-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n  <title>Solvathon Voice Agent</title>\n  <style>\n    :root{--bg:#0d1b26;--card:#132737;--fg:#e9f4fb;--muted:#9cb7ca;--brand:#1f9f8f;--danger:#c94d42}\n    *{box-sizing:border-box} body{margin:0;background:linear-gradient(135deg,#0b141d,#0f2333);color:var(--fg);font-family:Segoe UI,Arial,sans-serif;padding:18px}\n    .wrap{max-width:900px;margin:0 auto;display:grid;gap:14px}\n    .card{background:var(--card);border:1px solid rgba(255,255,255,.1);border-radius:14px;padding:16px}\n    h1{margin:0 0 8px} .sub{color:var(--muted);font-size:14px}\n    .grid{display:grid;grid-template-columns:1fr 1fr 1fr;gap:10px} label{font-size:12px;color:var(--muted);display:block;margin-bottom:6px}\n    input,select{width:100%;padding:10px;border-radius:10px;border:1px solid rgba(255,255,255,.2);background:#0b1a26;color:var(--fg)}\n    #holdBtn{width:100%;padding:16px;border:none;border-radius:12px;background:var(--brand);color:white;font-weight:700;font-size:16px;touch-action:none}\n    #holdBtn.active{background:var(--danger)}\n    .status{font-size:13px;color:var(--muted);margin-top:8px}\n    .chat{max-height:320px;overflow:auto;border:1px solid rgba(255,255,255,.15);border-radius:10px;padding:10px;background:#0b1a26}\n    .line{padding:8px;border-radius:8px;margin-bottom:8px}.u{background:rgba(31,159,143,.18)}.a{background:rgba(96,141,194,.18)}\n    @media (max-width: 760px){.grid{grid-template-columns:1fr}}\n  </style>\n</head>\n<body>\n  <div class=\"wrap\">\n    <div class=\"card\">\n      <h1>Solvathon Push-To-Talk</h1>\n      <div class=\"sub\">Hold button to record. Release to upload. Server replies with voice.</div>\n      <div class=\"grid\" style=\"margin-top:12px\">\n        <div><label>Backend URL</label><input id=\"backend\" /></div>\n        <div><label>Agent Purpose</label><select id=\"agentType\"></select></div>\n        <div><label>Language</label>\n          <select id=\"lang\">\n            <option value=\"en\">English</option>\n            <option value=\"hi\">Hindi</option>\n            <option value=\"ta\">Tamil</option>\n            <option value=\"te\">Telugu</option>\n            <option value=\"kn\">Kannada</option>\n            <option value=\"auto\">Auto Detect</option>\n          </select>\n        </div>\n      </div>\n      <div style=\"margin-top:12px\"><button id=\"holdBtn\">Hold To Talk</button></div>\n      <div class=\"status\" id=\"status\">Ready</div>\n    </div>\n\n    <div class=\"card\">\n      <div class=\"chat\" id=\"chat\"><div class=\"sub\">No turns yet.</div></div>\n    </div>\n  </div>\n\n  <audio id=\"player\" autoplay playsinline></audio>\n\n<script>\nconst backend = document.getElementById('backend');\nconst agentType = document.getElementById('agentType');\nconst lang = document.getElementById('lang');\nconst holdBtn = document.getElementById('holdBtn');\nconst statusEl = document.getElementById('status');\nconst chat = document.getElementById('chat');\nconst player = document.getElementById('player');\n\nlet stream=null, recorder=null, chunks=[], recording=false, busy=false, sessionId='';\nbackend.value = window.location.origin;\n\nfunction setStatus(s){statusEl.textContent=s}\nfunction line(text, cls){\n  if(chat.children.length===1 && chat.textContent.includes('No turns')) chat.innerHTML='';\n  const d=document.createElement('div'); d.className='line '+cls; d.textContent=text; chat.appendChild(d); chat.scrollTop=chat.scrollHeight;\n}\n\nasync function loadAgentTypes(){\n  try{\n    const res = await fetch(`${backend.value}/agent-types`);\n    const data = await res.json();\n    agentType.innerHTML='';\n    (data.agent_types||[]).forEach(a=>{\n      const o=document.createElement('option'); o.value=a.type; o.textContent=a.label; agentType.appendChild(o);\n    });\n  }catch(e){\n    agentType.innerHTML='<option value=\"hospital_kiosk\">Hospital Kiosk</option><option value=\"college_admission\">College Admission</option><option value=\"laptop_support\">Laptop Customer Support</option>';\n  }\n}\n\nfunction pickMime(){\n  const m=['audio/webm;codecs=opus','audio/webm','audio/mp4','audio/ogg;codecs=opus'];\n  for(const x of m){if(window.MediaRecorder && MediaRecorder.isTypeSupported(x)) return x;}\n  return '';\n}\n\nasync function ensureMic(){\n  if(stream) return stream;\n  stream = await navigator.mediaDevices.getUserMedia({audio:true});\n  return stream;\n}\n\nasync function sendTurn(blob, mime){\n  busy=true; holdBtn.disabled=true; setStatus('Uploading and processing...');\n  try{\n    const ext = mime.includes('mp4')?'m4a':mime.includes('ogg')?'ogg':'webm';\n    const form = new FormData();\n    form.append('audio', blob, `turn.${ext}`);\n    form.append('language', lang.value);\n    form.append('auto_detect', String(lang.value==='auto'));\n    form.append('agent_type', agentType.value);\n    if(sessionId) form.append('session_id', sessionId);\n\n    const res = await fetch(`${backend.value}/voice-turn`, {method:'POST', body:form});\n    const data = await res.json();\n    if(!res.ok) throw new Error(data.error || `HTTP ${res.status}`);\n\n    sessionId = data.session_id || sessionId;\n    if(data.transcript) line('You: '+data.transcript, 'u');\n    if(data.response) line((data.agent_name||'Agent')+': '+data.response, 'a');\n\n    if(data.audio_b64){\n      const bin = atob(data.audio_b64);\n      const arr = new Uint8Array(bin.length);\n      for(let i=0;i<bin.length;i++) arr[i]=bin.charCodeAt(i);\n      const url = URL.createObjectURL(new Blob([arr], {type:data.audio_mime || 'audio/wav'}));\n      player.src = url;\n      await player.play().catch(()=>{});\n    }\n\n    setStatus('Turn complete');\n  }catch(e){\n    setStatus('Error: '+e.message);\n    alert(e.message);\n  }finally{\n    busy=false; holdBtn.disabled=false; holdBtn.textContent='Hold To Talk'; holdBtn.classList.remove('active');\n  }\n}\n\nasync function startRec(ev){\n  ev.preventDefault();\n  if(busy || recording) return;\n  await ensureMic();\n  chunks=[];\n  const mime = pickMime();\n  recorder = mime ? new MediaRecorder(stream,{mimeType:mime}) : new MediaRecorder(stream);\n  recorder.ondataavailable = e => {if(e.data && e.data.size>0) chunks.push(e.data)};\n  recorder.onstop = async ()=>{\n    const useMime = recorder.mimeType || mime || 'audio/webm';\n    const blob = new Blob(chunks, {type:useMime});\n    chunks=[]; recorder=null;\n    if(blob.size<1200){setStatus('Audio too short'); return;}\n    await sendTurn(blob, useMime);\n  };\n  recorder.start(120);\n  recording=true;\n  holdBtn.classList.add('active');\n  holdBtn.textContent='Release To Send';\n  setStatus('Recording...');\n}\n\nfunction stopRec(ev){\n  if(ev) ev.preventDefault();\n  if(!recording || !recorder) return;\n  recording=false;\n  holdBtn.textContent='Uploading...';\n  if(recorder.state==='recording') recorder.stop();\n}\n\nholdBtn.addEventListener('pointerdown', startRec);\nholdBtn.addEventListener('pointerup', stopRec);\nholdBtn.addEventListener('pointerleave', stopRec);\nholdBtn.addEventListener('pointercancel', stopRec);\nholdBtn.addEventListener('contextmenu', e=>e.preventDefault());\n\nloadAgentTypes();\n</script>\n</body>\n</html>\n'''\n\napp = Flask(__name__)\nCORS(app)\n\n@app.get('/')\ndef root_page():\n    return INDEX_HTML\n\n@app.get('/healthz')\ndef healthz():\n    return jsonify({'ok': True, 'device': DEVICE})\n\n@app.get('/agent-types')\ndef list_agent_types():\n    payload = []\n    for k, v in AGENT_TYPES.items():\n        payload.append({'type': k, 'label': v['label'], 'description': v['assumption']})\n    return jsonify({'agent_types': payload})\n\n@app.post('/voice-turn')\ndef voice_turn_api():\n    if 'audio' not in request.files:\n        return jsonify({'error': \"Missing 'audio' file\"}), 400\n\n    audio_bytes = request.files['audio'].read()\n    if not audio_bytes:\n        return jsonify({'error': 'Empty audio file'}), 400\n\n    requested_language = (request.form.get('language') or 'auto').strip().lower()\n    requested_agent_type = (request.form.get('agent_type') or 'hospital_kiosk').strip().lower()\n    session_id = (request.form.get('session_id') or f'session_{int(time.time())}').strip()\n\n    try:\n        result = run_single_turn(\n            audio_bytes=audio_bytes,\n            language=requested_language,\n            agent_type=requested_agent_type,\n            session_id=session_id,\n        )\n    except Exception as e:\n        return jsonify({'error': f'Pipeline failed: {e}'}), 500\n\n    return jsonify({\n        'ok': True,\n        'session_id': session_id,\n        'agent_type': requested_agent_type,\n        'agent_name': AGENT_TYPES.get(requested_agent_type, AGENT_TYPES['hospital_kiosk'])['label'],\n        'language': result['language'],\n        'transcript': result['transcript'],\n        'response': result['response'],\n        'audio_b64': result['reply_audio_b64'],\n        'audio_mime': 'audio/wav',\n        'tts_engine': result['tts_engine'],\n    })\n\n\ndef run_server():\n    app.run(host='0.0.0.0', port=8080, debug=False, use_reloader=False)\n\nserver_thread = threading.Thread(target=run_server, daemon=True)\nserver_thread.start()\ntime.sleep(2)\nprint('Server started at: http://127.0.0.1:8080')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10) Expose Website With ngrok\n\n1. Create/get your token from [ngrok dashboard](https://dashboard.ngrok.com/get-started/your-authtoken).\n2. Paste token below and run this cell.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from pyngrok import ngrok\n\n# Set your token once in Colab session\nNGROK_AUTHTOKEN = 'PASTE_YOUR_NGROK_TOKEN'\nassert NGROK_AUTHTOKEN != 'PASTE_YOUR_NGROK_TOKEN', 'Please set NGROK_AUTHTOKEN first.'\n\nngrok.set_auth_token(NGROK_AUTHTOKEN)\npublic_tunnel = ngrok.connect(8080, bind_tls=True)\nprint('Public URL:', public_tunnel.public_url)\nprint('Open this URL in browser and use Hold To Talk.')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11) API Self-Test (inside notebook)\n\nThis sends a local sample audio file to `/voice-turn` and validates response fields.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import requests\n\nsample = TEST_AUDIO_DIR / 'lid_en.wav'\nif not sample.exists():\n    raise FileNotFoundError(f'Missing sample file: {sample}')\n\nwith open(sample, 'rb') as f:\n    files = {'audio': ('turn.wav', f, 'audio/wav')}\n    data = {\n        'language': 'auto',\n        'agent_type': 'hospital_kiosk',\n        'session_id': 'colab_self_test',\n    }\n    resp = requests.post('http://127.0.0.1:8080/voice-turn', files=files, data=data, timeout=300)\n\nprint('Status:', resp.status_code)\npayload = resp.json()\nprint(json.dumps({k: payload.get(k) for k in ['ok','language','agent_name','transcript','response','tts_engine']}, indent=2, ensure_ascii=False))\n\nif payload.get('audio_b64'):\n    audio_bytes = base64.b64decode(payload['audio_b64'])\n    out = OUTPUT_DIR / 'self_test_reply.wav'\n    out.write_bytes(audio_bytes)\n    print('Saved:', out)\n    display(Audio(filename=str(out), autoplay=False))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 12) What To Do Next\n\n1. Open the ngrok URL in mobile/desktop browser.\n2. Choose `Agent Purpose` and `Language` first.\n3. Hold button to speak.\n4. Release button to upload that spoken chunk.\n5. Listen to server-generated voice reply.\n\nIf you want to use your repo server directly (`src/realtime/signaling_server.py`), use:\n- `GET /push-to-talk`\n- `POST /voice-turn` with fields: `audio`, `language`, `auto_detect`, `agent_type`, `session_id`\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "name": "Solvathon_End_to_End_Colab.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}